#  勾配消失問題
誤差逆伝播法が下位層に進むにつれて、勾配がどんどん緩やかになる。
そのため、勾配降下法による更新では下位層のパラメータはほとんど変わらず、訓練は最適値に収束しなくなる。

## 活性化関数：シグモイド関数
勾配消失問題を引き起こすことがあった。理由は大きな値では出力の変化が微笑なため。
シグモイド関数を微分すると0.25が最大値になってしまう。

## 活性化関数：ReLU関数
勾配消失問題の回避とスパース化に貢献。

![gif latex](https://user-images.githubusercontent.com/85814165/140504207-a57ae6c6-9101-4777-8acc-36df49cc06ec.gif)

![スクリーンショット 2021-11-05 20 27 56](https://user-images.githubusercontent.com/85814165/140503816-fcf7f1bc-bbf8-46d2-a7c2-743fdc0adb3a.png)

## 重みの初期値設定
NNを学習始める時に、何箇所かある重みは何かしらのルールに基づいて作成する。多くの場合は乱数。

シグモイド関数に対しては、Xavier（サビエル）という乱数設定方法がある。
正規分布に従った乱数の生成。
前の層のノード数のルートで除算した値

ReLU関数には、重みの要素を、前の層のノード数の平方根で除算した値に対して√2をかけた値

## バッチ正規化
ミニバッチ単位で、入力値のデータの偏りを抑制する方法。
- 中間層の重みの更新が安定化する、学習スピードが向上する。
- 過学習を抑えられる。
GPU：1-64枚
TPU：1-256枚

ミニバッチの平均

![gif latex-2](https://user-images.githubusercontent.com/85814165/140510796-33890b32-ab9b-4097-bc36-628411724558.gif)

ミニバッチの分散

![gif latex-3](https://user-images.githubusercontent.com/85814165/140510897-62da32a8-6973-4f5f-9fab-a0c3d2cc0ee1.gif)

ミニバッチの正規化

![gif latex-4](https://user-images.githubusercontent.com/85814165/140511053-9bbc9cf4-ca25-43b1-934a-17c0bf0714da.gif)

変倍・移動

![gif latex-5](https://user-images.githubusercontent.com/85814165/140511113-2c52f975-d208-4a77-8149-8f4366ecf2af.gif)

γは定数倍、βはバイアス

# 確認テスト

Q.連鎖律の原理を使い、dz/dxを求めよ。z=t^2, t=x+y

![gif latex-101](https://user-images.githubusercontent.com/85814165/140500014-a2d854b5-6ceb-4572-aec8-f2a40fbf993e.gif)

Q.シグモイド関数を微分したとき、入力値が0の時に最大値をとる。その値として正しいものを選択肢から選べ

A.0.25が最大値

Q.重みの初期値に0を設定すると、どのような問題が発生するか。簡潔に説明せよ。

A.重みを0で初期化すると正しい学習が行えない。すべての重みの値が均一に更新されるため、多数の重みを持つ意味がなくなる。

Q.一般的に考えられるバッチ正規化の効果を2点あげよ。

A1.中間層の重みの更新が安定化する、学習スピードが向上する。
A2.過学習を抑えられる。
