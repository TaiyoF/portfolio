# 勾配降下法
- 勾配降下法
- 確率的勾配降下法
- ミニバッチ勾配降下法
NNを学習させる手法のこと。
学習を通じて、重み・バイアスを最適化する。
誤差関数を最小にする重み・バイアスを見つけていく。

## 勾配降下法
全サンプルの平均誤差

![gif latex-81](https://user-images.githubusercontent.com/85814165/139636911-3b776db4-b84b-4ca3-b7c3-cfed8bfdeac5.gif)

ε＝学習率

学習率が大きいと、発散する可能性がある。
学習率が小さいと、収束するまでに時間がかかってしまう。また、底が複数ある場合は、１つ目の底で収束してしまう可能性がある。（極小値をとってしまう）
適切な学習率を設定しないといけない。

## アルゴリズム
- Momentum
- AdaGrad
- Adadelta
- Adam（最も使われる）

## エポック
学習する回数 epoch

## 確率的勾配降下法
ランダムに抽出したサンプルの誤差
メリット：計算コストの軽減、局所的な極小解に収束すりリスクの軽減、オンライン学習ができる

![gif latex-82](https://user-images.githubusercontent.com/85814165/139641409-70244c9d-6a53-4364-8b30-6111eb1e55ab.gif)


## ミニバッチ勾配降下法



#  確認テスト

Q.該当するソースコードを探してみよう(勾配降下法）

![スクリーンショット 2021-11-01 16 43 35](https://user-images.githubusercontent.com/85814165/139638312-da81fea1-0afd-4219-98b8-b995f8de68c8.png)


Q.オンライン学習とはなにか２行でまとめよ

Q.この数式の意味を図に書いて説明せよ

Q.誤差逆伝播法では冬な再帰的処理を避けることができる。すでに行った計算結果を保持しているソースコードを抽出せよ

Q.２つの空欄に該当するソースコードを探せ

# 修了課題
